{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "import class_helper\n",
      "get_ipython().run_line_magic('aimport', 'class_helper')\n",
      "\n",
      "clh= class_helper.Classification_Helper()\n",
      "X_digits,  y_digits = clh.load_digits()\n",
      " no stored variable _latex_std_\n"
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "This is the \"trailer\" for the course: a brief plot summary and introduction to the key characters\n",
    "you will encounter.\n",
    "\n",
    "## Goals\n",
    "- Get a high level view of Machine Learning\n",
    "- Introduce notation\n",
    "- Preview concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classical ML and Deep Learning\n",
    "\n",
    "There are two main streams in this course\n",
    "- \"Classical ML\"\n",
    "    - somewhat long history\n",
    "    - somewhat related to Statistics\n",
    "- \"Deep Learning\"\n",
    "    - really took off after 2010\n",
    "    - more related to Artificial Intelligence than Statistics\n",
    "        - experimental versus mathematical\n",
    "        \n",
    "This preview is for Classical Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The big picture\n",
    " <img src=ml_taxonomy.png>\n",
    "\n",
    "- Types of learning (batch or online)\n",
    "    - Supervised\n",
    "    - Unsupervised\n",
    "    - Semi-supervised\n",
    "    - Reinforcement Learning\n",
    "- Types of targets\n",
    "    - continous\n",
    "    - discrete\n",
    "- Types of features\n",
    "    - numerical\n",
    "    - categorical\n",
    "    - text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnCAQKeGFAo0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "Supervised learning is about *informed prediction from labeled Data*.\n",
    "\n",
    "- Supervised Learning Algorithms\n",
    "    - k-Nearest Neigbors\n",
    "    - Linear Regression\n",
    "    - Logisitic Regression\n",
    "    - Support Vector Machines (SVMs)\n",
    "    - Decision Trees and Random Forests\n",
    "    - Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Classification: Predicting Discrete Labels\n",
    "<img src=ClassificationFig1.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Supervised Learning Result\n",
    "<img src=ClassificationFig2.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Prediction with trained model\n",
    "<img src=ClassificationFig2.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Regression: Predicting Continuous Labels\n",
    "<img src=RegressionFig1.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualize in 3D\n",
    "<img src=RegressionFig2.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Trained Model\n",
    "<img src=RegressionFig3.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Training data is unlabeled - Learn without a teacher\n",
    "\n",
    "- Unsupervised Learning Algorithms\n",
    "    - Clustering\n",
    "        - k-Means\n",
    "        - Gaussian Mixture Models\n",
    "        - Hierarchical Cluster Analysis (HCA)\n",
    "        - Expectation Maximization\n",
    "    - Visualization and dimensionality reduction\n",
    "        - Principal Component Analysis (PCA)\n",
    "        - Kernel PCA\n",
    "        - Manifold Learning\n",
    "        - Locally-Linear Embedding (LLE)\n",
    "        - t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    - Autoencoders are neural networks capable of\n",
    "        - Codings (lower dimension)\n",
    "        - Powerful Feature Detection (useful in pre-training stage of DNNs)\n",
    "        - Generative Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Clustering : Inferring labels on unlabeled data\n",
    "<img src=ClusteringFig1.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applying k-Means\n",
    "<img src=ClusteringFig2.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Dimensionality Reduction : Inferring Structure\n",
    "<img src=DimRedFig1.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applying Manifold Learning\n",
    "<img src=DimRedFig2.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's take a concrete example to get going\n",
    "\n",
    "- MNIST dataset\n",
    "<table>\n",
    "    <tr>\n",
    "        <center>Prediction: what digits do these pixels represent ?</center>\n",
    "    </tr>\n",
    "<img src=../images/mnist_small_test.png>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnCAQKeGFAo0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A single input $\\x$ is a vector of length $n$, i.e., a collection of $n$ *features*.\n",
    "\n",
    "- A **predictor** is a map from $\\x$ to a class (label) $\\hat{\\y}$.\n",
    "\n",
    "For now: \n",
    "- a class is drawn from a finite set $C$ of potential classes.\n",
    "- we are describing Classification -- mapping $\\x$ to a single class.\n",
    "- we will extend to Regression: outputs are from a continous universe (e.g., numbers)\n",
    "\n",
    "An example is a pair $(\\x, c)$ of a feature vector $\\x$ and a class $c \\in C$ (the target).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnCAQKeGFAo0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Consider a single example $(\\x,c)$.\n",
    "\n",
    "- A simple but naive predictor would map $\\x$ to a random $c' \\in C$.\n",
    "\n",
    "The probability of the predictor being correct ($c' = c$) is\n",
    "$\\frac{1}{||C||}$ .\n",
    "\n",
    "**Informed prediction** is when the probability of the predictor making a correct prediction\n",
    "is greater than $\\frac{1}{||C||}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnCAQKeGFAo0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we achieve this ?\n",
    "\n",
    "- Perhaps the individual features (elements of $\\x$) are associated with the correct class $c$.\n",
    "\n",
    "- The aim of Supervised Learning is to create a function (predictor) that maps an $\\x$ to the correct $c$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNAynOcdBplo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation\n",
    "\n",
    "- Supervised Learning involves supplying a number ($m$) of examples.\n",
    "- Each example is a vector $\\x$ consisting of $n \\ge 1$ *features* (attributes)\n",
    "and a scalar (sometimes a vector) $y$ which is the *target* value associated with this feature vector.\n",
    "\n",
    "- we use **bold face** to indicate a vector (e.g, $\\x$)\n",
    "- We use superscript $\\ip$ to denote an element $i$ of a collection of $m$ examples (e.g., $\\x^\\ip$)\n",
    "- We use subscript $j$ to index element $j$ of a vector, e.g., $\\x^\\ip_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNAynOcdBplo",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- So $\\x^\\ip$ is\n",
    "\n",
    "$\n",
    "  \\x^\\ip = \\begin{pmatrix}\n",
    " \\x^\\ip_1 \\\\\n",
    " \\x^\\ip_2 \\\\\n",
    "  \\vdots  \\\\\n",
    " \\x^\\ip_n\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "\n",
    "Each  element of $\\x^\\ip$ is a \"feature\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Not just numbers !\n",
    "\n",
    "The features *aren't restricted to be numeric* !\n",
    "\n",
    "In this course, we will deal with data that is\n",
    "- numeric\n",
    "- categorical\n",
    "- text\n",
    "- image\n",
    "- sound (not this course)\n",
    "\n",
    "Of course, you'll have to encode this data as numbers in order for numerical algorithms to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1441BMaCos9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training set\n",
    "\n",
    "- The collection of examples used for fitting (training) a model is called the *training set*:\n",
    "\n",
    "$$ \\langle \\X, \\y \\rangle= [ \\x^\\ip, \\y^\\ip | 1 \\le i \\le m ]$$\n",
    "\n",
    "where $m$ is the size of training set and each $\\x^\\ip$ is a feature vector of length $n$.\n",
    "\n",
    "- $\\X$ is an $(m \\times n)$ matrix and $\\y$ is an $(m \\times 1)$ vector of targets.\n",
    "\n",
    "\n",
    "$\n",
    "  \\X = \\begin{pmatrix}\n",
    "  (\\x^{(1)})^T \\\\\n",
    "  (\\x^{(2)})^T\\\\\n",
    "  \\vdots \\\\\n",
    "  (\\x^{(m)})^T \\\\\n",
    "  \\end{pmatrix} = \\begin{pmatrix}\n",
    " \\x^{(1)}_1 \\ldots\\x^{(1)}_n \\\\ \n",
    "  \\x^{(2)}_1 \\ldots\\x^{(2)}_n \\\\ \n",
    "   \\vdots \\\\\n",
    "  \\x^{(m)}_1 \\ldots\\x^{(m)}_n \\\\\n",
    "  \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>Training set</center>\n",
    "    </tr>\n",
    "<img src=../images/mnist_small_train.png>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1441BMaCos9",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We will sometimes add a \"constant\" feature by setting\n",
    "$\\x^\\ip_0 = 1,  0 \\le i \\le m$\n",
    "so that the first column of $\\x$ is $1$:\n",
    "\n",
    "$\n",
    "\\X =\n",
    "\\begin{pmatrix}\n",
    "  1  &\\x^{(1)}_1  & \\ldots &\\x^{(1)}_n \\\\ \n",
    "   1 &\\x^{(2)}_1  &\\ldots  &\\x^{(2)}_n \\\\ \n",
    "   \\vdots & \\vdots & \\ldots &  \\vdots \\\\\n",
    "   1 &\\x^{(m)}_1  &\\ldots  &\\x^{(m)}_n \\\\\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "\n",
    "- So each of the $m$ rows is an example and each of the $n$ columns is a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af60_0oNMM3n",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction\n",
    "\n",
    "- Given training example $\\x^\\ip$, we construct a function $h$ to predict its label\n",
    "\n",
    "$\\hat{\\y}^\\ip = h(\\x^\\ip)$\n",
    "- The function $h$ will often be parameterized (by $\\Theta$) so, for clarity, we should write\n",
    "\n",
    "$\\hat{\\y}^\\ip = h(\\x^\\ip; \\Theta)$\n",
    "- We will often drop $\\Theta$ for ease of reading.\n",
    "- Since $h$ is a function, it should also be possible to make a prediction for a vector $\\mathbf{x}$ that is **not** part of the training set.\n",
    "- That is, we are able *generalize* to non-training examples:  to make out of sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Making it concrete: Let's predict !\n",
    "\n",
    "Let's load a dataset to make these concepts concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import class_helper\n",
    "%aimport class_helper\n",
    "\n",
    "clh= class_helper.Classification_Helper()\n",
    "X_digits,  y_digits = clh.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's see what $m$ (number of examples), $n$ (number of features) are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=1797 training examples\n",
      "n=64 features per example\n",
      "10 classes: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"m={m:d} training examples\".format(m=X_digits.shape[0]))\n",
    "print(\"n={m:d} features per example\".format(m=X_digits.shape[1]))\n",
    "targets = np.unique(y_digits)\n",
    "targets.sort()\n",
    "\n",
    "print(\"{nc:d} classes: {c:s}\".format(nc=len(targets), c=\", \".join( [ str(t) for t in targets ]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 0, range(0.00, 1.00):\n",
      "\t  [0.     0.     0.3125 0.8125 0.5625 0.0625 0.     0.     0.     0.\n",
      " 0.8125 0.9375 0.625  0.9375 0.3125 0.     0.     0.1875 0.9375 0.125\n",
      " 0.     0.6875 0.5    0.     0.     0.25   0.75   0.     0.     0.5\n",
      " 0.5    0.     0.     0.3125 0.5    0.     0.     0.5625 0.5    0.\n",
      " 0.     0.25   0.6875 0.     0.0625 0.75   0.4375 0.     0.     0.125\n",
      " 0.875  0.3125 0.625  0.75   0.     0.     0.     0.     0.375  0.8125\n",
      " 0.625  0.     0.     0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Across the features of all examples: what is the min and the max ?\n",
    "# Let's look at the feature vector for example at index ex_num\n",
    "ex_num = 0\n",
    "print(\"\\nExample {n:d}, range({mn:2.2f}, {mx:2.2f}):\\n\\t \".format(n=ex_num, \n",
    "                                                              mn=X_digits.min(), mx=X_digits.max()\n",
    "                                                             ),\n",
    "      X_digits[ex_num,:]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The dataset contains a number of examples.\n",
    "\n",
    "    - Each example $\\x^\\ip$ is a vector of 64 features, which are numbers in the range $[0,1]$\n",
    "    - The target $y^\\ip$ is a digit in the range $[0,9]$\n",
    "\n",
    "- In other words: the examples are encodings of images with labels that indicate what the image is.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the examples are grey scale values, we can re-arrange them into a square grid and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = clh.plot_digits(X_digits,  y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Our problem is to take an unknown $\\x$ and map it (predict) to a label in the range $[0,9]$.\n",
    "\n",
    "- This is a *classification* problem as our predictions are from a finite set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN score: 0.990000\n",
      "LogisticRegression score: 1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xd_train, Xd_test, yd_train, yd_test, models = clh.fit_digits(X_digits, y_digits)\n",
    "\n",
    "_= clh.predict_digits(models[\"knn\"], Xd_test[:10], yd_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- How would **you** predict a label for an image, given the 64 pixel values ?\n",
    "\n",
    "- We will use a very simple (and inefficient) algorithm called *K Nearest Neighbors (KNN)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WAwEIaJ9UmAC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Template matching\n",
    "\n",
    "$\\newcommand{\\vc}{\\mathbf{v}_{(c)}}$\n",
    "\n",
    "- One approach to Classification is to match our input vector $\\x$ against a *template*:\n",
    "(a vector of similar length) whose class is known.\n",
    "\n",
    "- With one template $\\vc$ for each class $\\c \\in C$, we could classify $\\x$ as being in the class $c'$\n",
    "whose template was \"closest\" to $\\x$.\n",
    "\n",
    "- We need a similarity measure that maps $\\x$ and $\\vc$ to a number such that\n",
    "larger means more similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuTufoJG2R6N",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our first predictor: K Nearest Neighbors (KNN)\n",
    "\n",
    "- Here's one of the simplest Machine Learning algorithms, that leverages template matching.\n",
    "- In this case, the templates are the feature vectors of the training set.\n",
    "\n",
    "- Use the similarity measure to find the $K$ training examples closest to $\\x$; \n",
    "\n",
    "- Predict the class that appears most frequently among these $K$ examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuTufoJG2R6N",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Here is our predictor function, given *test* input $\\x$:\n",
    "    - For each training example $\\x^\\ip$, compute the similarity $s^\\ip$ of $\\x$ to $\\x^\\ip$\n",
    "    - Let $S_K$ be the set of $K$ training examples $i_1, i_2, \\ldots, i_K$with greatest similarity to $\\x$\n",
    "        - $Y = [ \\y^{(j)}  | \\ j \\in S_K ]$ be the classes associated with these closest examples\n",
    "    - Let $\\text{count}_c$ be the number of elements of $Y$ that are equal to class $c, c \\in C$.\n",
    "    - Predict class $c'$, with the greatest $\\text{count}_{c'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuTufoJG2R6N",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- KNN operates under the assumption (Manifold Hypothesis) that if two vectors are similar, they have the same class.\n",
    "\n",
    "- If $K=1$, the predictions are highly sensitive to the training examples; increasing $K$ may increase\n",
    "the prediction accuracy.\n",
    "  \n",
    "- Although simple, can you spot the drawback to KNN ?\n",
    "\n",
    "    - The size of $\\Theta$ (the number of parameters) is proportional to\n",
    "        - the size of the training set: $m * n$\n",
    "        - ideally: $m$ is very large\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "KNN is so simple it's almost embarassing to call it Machine Learning.\n",
    "But it does illustrate the key steps\n",
    "- the basis of Supervised Learning are training examples\n",
    "    - the more the better\n",
    "- the training examples are used to *fit* a predictor\n",
    "    - we will learn many predictors (models) in this course\n",
    "- the features of the examples are the key to prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- KNN did not make intelligent use of the features: it merely  memorized the $m$ examples.\n",
    "\n",
    "    - That is, it used $m$ templates each of size $n$ so $|\\Theta| = m*n$.\n",
    "\n",
    "- We will see that many ML algorithms, both Classic (e.g., Regression) and Deep Learning,\n",
    "are based on *solving* for $\\Theta$ -- finding small templates that are effective\n",
    "for prediction.\n",
    "\n",
    "- A more intelligent basis for prediction would include:\n",
    "    - finding one (or more) features that are predictive\n",
    "    - finding relationships among features that are predictive\n",
    "    - find a subset of features that is *common across all examples* in a class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Another issue: perhaps we are using the wrong features ?\n",
    "    - are $n = 64$ raw pixels the best representation of the input (and template) for learning ?\n",
    "    - would higher level features (e.g., groups of pixels that form horizontal/vertical lines) be more efficient ?\n",
    "\n",
    "\n",
    "- This is called Data Transformation or Feature Engineering and will be key concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting a predictor (training a model)\n",
    "\n",
    "- As simple as KNN is, it has far less mathematical basis than most ML algorithms.\n",
    "\n",
    "- To be more formal, a predictor is a function of feature vectors $\\x$ whose behavior\n",
    "is parameterized by $\\Theta$.\n",
    "    - terminology: also called an *estimator*, *fitted model*\n",
    "\n",
    "- An *objective function*  measures how well the predictor performs\n",
    "on the training set, given  $\\Theta$.\n",
    "\n",
    "- Fitting (or training) the predictor: solve for the $\\Theta$\n",
    "that maximizes the objective function.\n",
    "\n",
    "**Note** The objective function is relative to the *training* set; it is usually similar, but not\n",
    "identical, to the Performance Measure that quantifies how well the predictor peforms\n",
    "out of sample (e.g., on the Test or Validation set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6lKaQvuHCc1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cost/Loss, Utility\n",
    "\n",
    "- The prediction $\\hat{\\y}^{(i)}$ for example $\\x^\\ip$ is perfect if it matches the true label $\\y^\\ip$\n",
    "\n",
    "$ \\hat{\\y}^\\ip = \\y^\\ip$\n",
    "\n",
    "- Perfection  is hard (at least at first) so we need a measure for \"how far off\" the prediction is.\n",
    "\n",
    "- We will call the distance between $\\hat{\\y}^{(i)}, y^\\ip$ the *Loss* (or *Cost*) for example $i$:\n",
    "\n",
    "$$\n",
    "\\loss^\\ip_\\Theta =  L( \\;  h(\\x^\\ip; \\Theta),  \\y^\\ip \\;) = L( \\hat{\\y}^\\ip , \\y) \n",
    "$$\n",
    "\n",
    "where $L(a,b)$ is a function that is $0$ when $a = b$ and increasing as $a$ increasingly differs from $b$.\n",
    "\n",
    "Two common forms of $L$ are Mean Squared Error (for Regression) and Cross Entropy Loss (for classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6lKaQvuHCc1",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Loss for the entire training set is simply the average (across examples) of the Loss for the example\n",
    "\n",
    "$$\n",
    "\\loss_\\Theta  = { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whereas Loss describes how \"bad\" our prediction is, we sometimes refer to the converse -- how \"good\" the prediction is.\n",
    "\n",
    "We call the \"goodness\" of the prediction  the *Utility* $U_\\Theta$.\n",
    "\n",
    "So we could state the optimization objective either as\n",
    "\"minimize Cost\" or \"maximize Utility\".\n",
    "\n",
    "By convention, the DL optimization problem is usually framed as one of minimization (of cost or loss) \n",
    "rather than maximization of utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since Cost is inversely related to Utility, you will sometimes see\n",
    "the minimization objective written as\n",
    "\"minimize -1 times Utility\".\n",
    "\n",
    "So be forewarned that you will often see Loss function with leading \"negation\" signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6lKaQvuHCc1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating Loss functions is a key part of Deep Learning\n",
    "\n",
    "As you will come to see, particularly for Deep Learning, the essence of many problems is in creating a Loss Function that captures the objective of your problem.\n",
    "\n",
    "This is  far from a trivial part of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHoiCEeMRxG-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization: Minimize Cost/Loss, Maximize Utility\n",
    "\n",
    "- The goal of fitting/training is to solve for the $\\Theta$ that minimizes the training set loss \n",
    "$L_\\Theta$ (or conversely, maximizes the Utility $U_\\Theta$.\n",
    "\n",
    "- The method for finding $\\Theta$ is called optimization.\n",
    "\n",
    "- There is one optimization method that we will study in depth: Gradient Descent.\n",
    "\n",
    "- We can use this in Classical ML but it will become a key tool once we move on to Deep Learning.\n",
    "\n",
    "- One focus of this course will be variations on Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHoiCEeMRxG-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The difficulty of finding a good \"solution\" to a problem is\n",
    "    - creating a loss function that describes your objectives, which often have multiple aspects\n",
    "    - having a large and diverse set of training examples to estimate $\\Theta$.\n",
    "\n",
    "- Many packages have a method \"fit\" that takes the training set and performs the optimization.\n",
    "\n",
    "- These packages usually create a \"model\" object (containing the $\\Theta$ among other things)\n",
    "\n",
    "- We use *predictor* and *model* as synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aojGEqYcw9Z5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The dot product: a common Utility function\n",
    "\n",
    "- The \"dot product\" (special case of inner product) is one function\n",
    "that often appears in template matching\n",
    "\n",
    "- It measures the\n",
    "similarity of two vectors\n",
    "\n",
    "$\n",
    "\\mathbf{v} \\cdot \\mathbf{v}' = \\sum_{i=1}^n \\mathbf{v}_i \\mathbf{v}'_i\n",
    "$\n",
    "\n",
    "- As a similarity measure (rather than as a distance) high dot product means \"more similar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aojGEqYcw9Z5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are several intuitions for the dot product\n",
    "\n",
    "- The dot product is maximized  when large (resp., small) values appear in similar positions in both vectors\n",
    "  - this becomes evern more obvious if we $0$-center both vectors such that \"small\" values become negative\n",
    "  - this looks like the statistical formula for covariance\n",
    "    - if we normalize both vectors to unit length, then this looks like correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aojGEqYcw9Z5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " - Geometric.  Let $\\alpha$ be the angle between vectors.\n",
    "    - $\\mathbf{v} \\cdot \\mathbf{v}' =  || \\mathbf{v} || *  || \\mathbf{v}' || * \\cos(\\alpha)$\n",
    "    - $\\mathbf{v} \\cdot \\mathbf{v}' \\over{ || \\mathbf{v} || *  || \\mathbf{v}' ||}$ is called the cosine similarity\n",
    "      - similarity between normalized vectors\n",
    "    - similarity is maximized when $\\alpha = 0$, that is $v$ and $v'$ are coincident (but perhaps different lengths)\n",
    "    - similarity is $0$ when $v, v'$ are orthogonal\n",
    "    - similarity is negative when $v, v'$ point in different directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aojGEqYcw9Z5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can generalize dot product to higher dimensions by taking the sum of element-wise multiplication (or simply by first flattening both vectors to one dimension)\n",
    "\n",
    "- We will see the dot product appear repeatedly, particularly in Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges of ML\n",
    "\n",
    "- You need data to train, often a lot of it\n",
    "    - not always easy to get\n",
    "        - supervised: needs to be labelled\n",
    "    - quality issues\n",
    "    - Is the training data representative of \"the real world\" for which you are designing ?\n",
    "- Overfitting and Underfitting\n",
    "    - Overfit: good training accuracy, poor generalization\n",
    "    - Underfit: lost opportunity\n",
    "- Engineering meaningful features is key \n",
    "    - Data transformations\n",
    "        - create features that aid prediction\n",
    "        - art and science\n",
    "- Testing and validation\n",
    "    - an honest test uses held-out data\n",
    "    - training data is a precious resource; painful to hold some out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# No Free Lunch Theorem\n",
    "\n",
    "- In absence of information about data, there is no one apriori model guaranteed to work better.\n",
    "- In practice\n",
    "    - Make reasonable assumptions on data\n",
    "    - Evaluate a few reasonable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML in one slide\n",
    "<img src=../images/Cartoon_ML.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Machine Learning is a *process* that involves multiple steps\n",
    "    - It is *not* just learning to use various models (predictors)\n",
    "    - We will emphasize the process as much as the algorithms\n",
    "- Supervised Machine Learning depends on the availability of data\n",
    "    - obtaining, cleaning, augmenting data is important\n",
    "- An example is a collection of \"features\"\n",
    "    - finding/creating/interpretting features is the key skill of a Data Scientist\n",
    "        - which features are important \n",
    "        - how do features interact\n",
    "    - sometimes features are missing or too low level\n",
    "    - a key skill is creating features than enable learning\n",
    "    ML\n",
    "\n",
    "\n",
    "- A key part of Machine Learning is stating an optimization objective that captures your goal\n",
    "    - not always obvious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=images/ML_process.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Overview_and_notation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
